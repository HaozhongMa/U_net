{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% import\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug=True\n"
     ]
    }
   ],
   "source": [
    "__debug__==True\n",
    "print(\"debug=%s\"%__debug__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-e9fde488993a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mDownsampleLayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mconv1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConv2D\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m64\u001B[0m \u001B[1;33m,\u001B[0m\u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'same'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mconv2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConv2D\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m64\u001B[0m \u001B[1;33m,\u001B[0m\u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'same'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mpool\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMaxPooling2D\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpool_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__int__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munits\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class DownsampleLayer(tf.keras.layers.Layer):\n",
    "    conv1 = tf.keras.layers.Conv2D(64 ,kernel_size=3, padding='same')\n",
    "    conv2 = tf.keras.layers.Conv2D(64 ,kernel_size=3, padding='same')\n",
    "    pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "    def __int__(self, units):\n",
    "        \"\"\"\n",
    "        units,cov2d的filters参数？ 这个参数的主要含义是什么？\n",
    "        :param units:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(DownsampleLayer, self).__int__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(units,\n",
    "                                            kernel_size=3,\n",
    "                                            padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(units,\n",
    "                                            kernel_size=3,\n",
    "                                            padding='same')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    def call(self, inputs, is_pool=True):\n",
    "        \"\"\"\n",
    "        layers类的call方法的重新定义？\n",
    "        :param inputs:\n",
    "        :param is_pool:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if is_pool:\n",
    "            inputs = self.pool(inputs)\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        outputs = tf.nn.relu(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class UpsampleLayer(tf.keras.layers.Layer):\n",
    "    def __int__(self, units):\n",
    "        super(UpsampleLayer, self).__int__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(units, kernel_size=3,\n",
    "                                            padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(units, kernel_size=3,\n",
    "                                            padding='same')\n",
    "        self.deconv = tf.keras.layers.Conv2DTranspose(units // 2,\n",
    "                                                      kernel_size=2,\n",
    "                                                      strides=2,\n",
    "                                                      padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.deconv(x)\n",
    "        outputs = tf.nn.relu(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class UnetModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    定义模型，这个模型的父类是来自keras的Model类\n",
    "    \"\"\"\n",
    "\n",
    "    def __int__(self):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.down_sample_I = DownsampleLayer(64)\n",
    "        self.down_sample_II = DownsampleLayer(128)\n",
    "        self.down_sample_III = DownsampleLayer(256)\n",
    "        self.down_sample_IV = DownsampleLayer(512)\n",
    "        self.down_sample_V = DownsampleLayer(1024)\n",
    "        self.up_sample = tf.keras.layers.Conv2DTranspose(512,\n",
    "                                                         kernel_size=2,\n",
    "                                                         strides=2,\n",
    "                                                         padding='same')\n",
    "        self.up_sample_I = UpsampleLayer(512)\n",
    "        self.up_sample_II = UpsampleLayer(256)\n",
    "        self.up_sample_III = UpsampleLayer(128)\n",
    "        self.conv_last = UpsampleLayer(64)\n",
    "        self.last = tf.keras.layers.Conv2D(2,\n",
    "                                           kernel_size=1,\n",
    "                                           padding='same')\n",
    "\n",
    "    def call(self,inputs):\n",
    "        #下采样5层\n",
    "        x1 = self.down_sample_I(inputs,is_pool=False)\n",
    "        x2 = self.down_sample_II(x1)\n",
    "        x3 = self.down_sample_III(x2)\n",
    "        x4 = self.down_sample_IV(x3)\n",
    "        x5 = self.down_sample_V(x4)\n",
    "        #上采样5层\n",
    "        x5 = self.up_sample(x5)\n",
    "        x5 = tf.concat([x4,x5])\n",
    "        x5 = self.up_sample_I(x5)\n",
    "        x5 = tf.concat([x3,x5])\n",
    "        x5 = self.up_sample_II(x5)\n",
    "        x5 = tf.concat([x2,x5])\n",
    "        x5 = self.up_sample_III(x5)\n",
    "        x5 = tf.concat([x1,x5])\n",
    "        x5 = self.conv_last(x5,is_pool=False)\n",
    "        x5 = self.last(x5)\n",
    "        return x5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 定义UnetModel类\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 88: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_FallbackException\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001B[0m in \u001B[0;36mread_file\u001B[1;34m(filename, name)\u001B[0m\n\u001B[0;32m    548\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ReadFile\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 549\u001B[1;33m         tld.op_callbacks, filename)\n\u001B[0m\u001B[0;32m    550\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31m_FallbackException\u001B[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-8e80a5c8d7e3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     84\u001B[0m     \u001B[0mimg_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mr\"C:\\work\\UnetCT\\CT_data\\tran\\imgs\\0.png\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[0mmask_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mr\"C:\\work/UnetCT\\CT_data\\tran\\masks\\0.png\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 86\u001B[1;33m     \u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mload_test_image\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmask_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     87\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-18-8e80a5c8d7e3>\u001B[0m in \u001B[0;36mload_test_image\u001B[1;34m(img_path, mask_path)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \"\"\"\n\u001B[0;32m     74\u001B[0m     \u001B[1;31m#读取数据\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_images_png\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m     \u001B[0mmask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_masks_png\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m     \u001B[1;31m#直接压缩数据到（256,256,3），（256,256,1）\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-18-8e80a5c8d7e3>\u001B[0m in \u001B[0;36mread_images_png\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[1;32mreturn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \"\"\"\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode_png\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mchannels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001B[0m in \u001B[0;36mread_file\u001B[1;34m(filename, name)\u001B[0m\n\u001B[0;32m    552\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    553\u001B[0m         return read_file_eager_fallback(\n\u001B[1;32m--> 554\u001B[1;33m             filename, name=name, ctx=_ctx)\n\u001B[0m\u001B[0;32m    555\u001B[0m       \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_SymbolicException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    556\u001B[0m         \u001B[1;32mpass\u001B[0m  \u001B[1;31m# Add nodes to the TensorFlow graph.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\u001B[0m in \u001B[0;36mread_file_eager_fallback\u001B[1;34m(filename, name, ctx)\u001B[0m\n\u001B[0;32m    590\u001B[0m   \u001B[0m_attrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    591\u001B[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001B[1;32m--> 592\u001B[1;33m                              attrs=_attrs, ctx=ctx, name=name)\n\u001B[0m\u001B[0;32m    593\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0m_execute\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmust_record_gradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    594\u001B[0m     _execute.record_gradient(\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001B[0;32m     60\u001B[0m                                                \u001B[0mop_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m                                                num_outputs)\n\u001B[0m\u001B[0;32m     62\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd5 in position 88: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "def read_images_png(path):\n",
    "    \"\"\"\n",
    "    读取数据文件\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img,channels=3)\n",
    "    return img\n",
    "\n",
    "def read_masks_png(path):\n",
    "    \"\"\"\n",
    "    读取mask文件\n",
    "    :param path:\n",
    "    :return: (rows,cols,1)\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img,channels=1)\n",
    "    return img\n",
    "\n",
    "#数据增强\n",
    "def crop_img(img,mask):\n",
    "    \"\"\"\n",
    "    叠放后裁剪、翻转\n",
    "    :param img:\n",
    "    :param mask:\n",
    "    :return: (256,256,3),(256,256,1)\n",
    "    \"\"\"\n",
    "    concat_img = tf.concat([img,mask],axis = -1)\n",
    "    concat_img = tf.image.resize(concat_img,(280,280),\n",
    "                                method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    cropped_img = tf.image.random_crop(concat_img,[256,256,4])\n",
    "    print(\"shape of cropped_img is %d\"%cropped_img.shape)\n",
    "    return cropped_img[ :, :, :3], cropped_img[ :, :, 3:]\n",
    "\n",
    "def normal(img,mask):\n",
    "    \"\"\"\n",
    "    对数据进行正态化处理，让值成为[-127.5,0,127.5]的分布状态\n",
    "    :param img:\n",
    "    :param mask:\n",
    "    :return: img(rows,cols,layers=3)(float32),mask(rows,cols,layers=1)(int32)\n",
    "    \"\"\"\n",
    "    img = tf.cast(img, tf.float32)/127.5 - 1\n",
    "    mask = tf.cast(mask/255, tf.int32)\n",
    "    return img, mask\n",
    "\n",
    "def load_train_image(img_path,mask_path):\n",
    "    \"\"\"\n",
    "    读取训练数据集，将图像和mask 合并以后，进行处理\n",
    "    :param img_path:\n",
    "    :param mask_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #读取数据\n",
    "    img = read_images_png(img_path)\n",
    "    mask = read_masks_png(mask_path)\n",
    "    #裁剪\n",
    "    img,mask = crop_img(img,mask)\n",
    "    #翻转\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    img_f32,mask_int32 = normal(img,mask)\n",
    "    return img_f32,mask_int32\n",
    "\n",
    "def load_test_image(img_path,mask_path):\n",
    "    \"\"\"\n",
    "    读取测试数据集，这个函数\n",
    "    todo:修改错误的部分，没有经过280,280的压缩，恐怕会有问题\n",
    "    :param img_path:\n",
    "    :param mask_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #读取数据\n",
    "    img = read_images_png(img_path)\n",
    "    mask = read_masks_png(mask_path)\n",
    "    #直接压缩数据到（256,256,3），（256,256,1）\n",
    "    img = tf.image.resize(img,(256,256))\n",
    "    mask = tf.image.resize(mask,(256,256))\n",
    "    img_f32,mask_int32 = normal(img,mask)\n",
    "    return img_f32, mask_int32\n",
    "\n",
    "if __debug__==True:\n",
    "    img_path=r\"C:\\work\\UnetCT\\CT_data\\tran\\imgs\\0.png\"\n",
    "    mask_path=r\"C:\\work/UnetCT\\CT_data\\tran\\masks\\0.png\"\n",
    "    img,mask=load_test_image(img_path,mask_path)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% 定义数据预处理函数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-13-661050eb969c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mimages_train_filelist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mglob\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mglob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr'C:/work/UnetCT/CT_data\\train\\imgs\\*.png'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "images_train_filelist = glob.glob(r'C:/work/UnetCT/CT_data\\train\\imgs\\*.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#训练数据列表\n",
    "#train数据集位置\n",
    "images_train_filelist = glob.glob(r'CT_data\\train\\imgs\\*.png')\n",
    "masks_train_filelist = glob.glob(r'CT_data\\train\\masks\\*.png')\n",
    "train_count = len(images_train_filelist)\n",
    "#重新排列数据的顺序\n",
    "index = np.random.permutation(train_count)\n",
    "images_train_filelist = np.array(images_train_filelist)[index]\n",
    "masks_train_filelist = np.array(masks_train_filelist)[index]\n",
    "\n",
    "#验证数据列表\n",
    "#test数据集位置\n",
    "images_test_filelist = glob.glob(r'CT_data\\valid\\imgs\\*.png')\n",
    "masks_test_filelist = glob.glob(r'CT_data\\valid\\masks\\*.png')\n",
    "test_count = len(images_test_filelist)\n",
    "\n",
    "#定义训练参数\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "BUFFER_SIZE = 10\n",
    "step_per_epoch = train_count//BATCH_SIZE\n",
    "test_step = test_count//BATCH_SIZE\n",
    "auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\"\"\"思路\n",
    "Step0: 准备要加载的numpy数据\n",
    "Step1: 使用 tf.data.Dataset.from_tensor_slices() 函数进行加载\n",
    "Step2: 使用 shuffle() 打乱数据\n",
    "Step3: 使用 map() 函数进行预处理\n",
    "Step4: 使用 batch() 函数设置 batch size 值\n",
    "Step5: 根据需要 使用 repeat() 设置是否循环迭代数据集\n",
    "\"\"\"\n",
    "# Step1: 使用 tf.data.Dataset.from_tensor_slices() 函数进行加载\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train_filelist,masks_train_filelist)) #step1,4,5\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test_filelist,masks_test_filelist))\n",
    "# Step2: 使用 shuffle() 打乱数据\n",
    "dataset_train = dataset_train.cache().repeat().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(auto)\n",
    "# Step3: 使用 map() 函数进行预处理\n",
    "dataset_train = dataset_train.map(load_train_image,num_parallel_calls = auto)\n",
    "dataset_test = dataset_test.map(load_test_image,num_parallel_calls = auto)\n",
    "# Step4: 使用 batch() 函数设置 batch size 值\n",
    "dataset_test = dataset_test.cache().batch(BATCH_SIZE)\n",
    "# Step5: 根据需要 使用 repeat() 设置是否循环迭代数据集\n",
    "#已经在step1做过了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%训练\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-3dce8eb68b7a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__debug__\u001B[0m\u001B[1;33m==\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mm\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdataset_train\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m         \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m         \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "if __debug__==True:\n",
    "    for i, m in dataset_train.take(1):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow((i.numpy()+1)/2)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(np.squeeze(m.numpy()))\n",
    "        plt.imshow(m.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%测试是否定义成功\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UnetModel' object has no attribute 'down_sample_I'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-5785bc0e5dc3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mUnetModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m256\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msummary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\work\\unetct\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mbuild\u001B[1;34m(self, input_shape)\u001B[0m\n\u001B[0;32m    417\u001B[0m                            'method accepts an `inputs` argument.')\n\u001B[0;32m    418\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 419\u001B[1;33m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    420\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001B[1;32m<ipython-input-5-6c7652e74333>\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m     79\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[1;31m#下采样5层\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m         \u001B[0mx1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdown_sample_I\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mis_pool\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m         \u001B[0mx2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdown_sample_II\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[0mx3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdown_sample_III\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'UnetModel' object has no attribute 'down_sample_I'"
     ]
    }
   ],
   "source": [
    "model = UnetModel()\n",
    "model.build(input_shape=(256,256,3))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%训练步骤\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "class MeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __call__(self,y_ture,y_pred,sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred,axis=-1)\n",
    "        return super().__call__(y_ture,y_pred,sample_weight=sample_weight)\n",
    "\n",
    "#定义训练步骤\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "train_iou = MeanIoU(2,name='train_iou')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n",
    "test_iou = MeanIoU(2,name='test_iou')\n",
    "\n",
    "@tf.function\n",
    "def train_step(images,labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        #需要看看这步有没有问题，这里变量名称定义的有点混乱\n",
    "        loss = loss(labels,predictions)\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels,predictions)\n",
    "    train_iou(labels,predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images,labels):\n",
    "    predictions= model(images)\n",
    "    t_loss=loss_object(labels,predictions)\n",
    "    test_loss(t_loss)\n",
    "\n",
    "    test_accuracy(labels,predictions)\n",
    "    test_iou(labels,predictions)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #训练一个epoch的数据\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_iou.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    test_iou.reset_states()\n",
    "    #训练\n",
    "    for images,labels in dataset_train: #这个应该是一张图片放进去训练吧\n",
    "        train_step(images,labels)\n",
    "    #验证\n",
    "    for test_images,test_labels in dataset_test:\n",
    "        train_step(test_images,test_labels)\n",
    "    #输出\n",
    "    template ='EPOCH {:.3f}, Loss:{:.3f}, Accuracy: {:.3f},\\\n",
    "               IOU:{:.3f},Test Loss:{:.3f},\\\n",
    "               Test Acurracy:{:.3f},Test IOU:{:.3f}'\n",
    "    \n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100,\n",
    "                           train_iou.result(),\n",
    "                           test_loss.result(),\n",
    "                           test_accuracy.result()*100,\n",
    "                           test_iou.result()\n",
    "                         ))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%定义训练过程中使用的函数\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}